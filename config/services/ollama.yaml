service: ollama
enabled: false
base_url: http://localhost:11434
model: llama2
temperature: 0.1
timeout: 120
keep_alive: 5m
models:
  available:
  - llama2
  - codellama
  - mistral
  default: llama2
environments:
  development:
    enabled: true
  production:
    enabled: false
  testing:
    enabled: false
