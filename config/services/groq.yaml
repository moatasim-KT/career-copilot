# Groq Service Configuration
service: groq
enabled: true

# Model Configuration
model: llama-3.1-8b-instant
temperature: 0.1
max_tokens: 4000
timeout: 30

# API Configuration
base_url: "https://api.groq.com/openai/v1"

# Available Models
available_models:
  - "llama-3.1-8b-instant"
  - "llama-3.1-70b-versatile"
  - "mixtral-8x7b-32768"
  - "gemma-7b-it"

# Retry Configuration
retry:
  max_attempts: 3
  backoff_factor: 1.5
  initial_delay: 0.5
  max_delay: 30.0

# Rate Limiting
rate_limit:
  requests_per_minute: 30
  tokens_per_minute: 6000

# Performance Configuration
performance:
  streaming: true
  batch_size: 1
  
# Environment Overrides
environments:
  development:
    model: "llama-3.1-8b-instant"
  production:
    model: "llama-3.1-70b-versatile"
  testing:
    enabled: false
