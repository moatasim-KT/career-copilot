# Alertmanager Configuration for Production Monitoring
# =============================================================================

global:
  # SMTP configuration for email alerts
  smtp_smarthost: 'smtp.company.com:587'
  smtp_from: 'alerts@company.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: '${SMTP_PASSWORD}'
  smtp_require_tls: true

# Templates for alert formatting
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route configuration
route:
  group_by: ['alertname', 'service', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default-receiver'
  
  routes:
    # Critical alerts - immediate escalation
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      group_interval: 5s
      repeat_interval: 5m
      
    # Security alerts - security team
    - match:
        team: security
      receiver: 'security-team'
      group_wait: 30s
      repeat_interval: 30m
      
    # Database alerts - database team
    - match:
        service: database
      receiver: 'database-team'
      group_wait: 1m
      repeat_interval: 15m
      
    # Business logic alerts - product team
    - match_re:
        alertname: '.*Processing.*|.*Business.*'
      receiver: 'business-team'
      group_wait: 5m
      repeat_interval: 2h
      
    # Infrastructure alerts - platform team
    - match:
        team: platform
      receiver: 'platform-team'
      group_wait: 2m
      repeat_interval: 30m

# Inhibition rules to prevent alert spam
inhibit_rules:
  # Inhibit all other alerts when service is completely down
  - source_match:
      alertname: ServiceCompletelyDown
    target_match_re:
      alertname: '.*'
    equal: ['service']
    
  # Inhibit high error rate when service is down
  - source_match:
      alertname: ServiceDown
    target_match:
      alertname: HighErrorRate
    equal: ['service']
    
  # Inhibit resource alerts when node is down
  - source_match:
      alertname: NodeDown
    target_match_re:
      alertname: 'High.*Usage|.*ResourceExhaustion'
    equal: ['instance']

# Receiver configurations
receivers:
  # Default receiver for non-critical alerts
  - name: 'default-receiver'
    email_configs:
      - to: 'team@company.com'
        subject: '{{ .GroupLabels.alertname }} - {{ .GroupLabels.service }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Service: {{ .Labels.service }}
          {{ if .Annotations.runbook_url }}
          Runbook: {{ .Annotations.runbook_url }}
          {{ end }}
          {{ end }}

  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@company.com'
        subject: 'üö® CRITICAL ALERT: {{ .GroupLabels.alertname }}'
        body: |
          CRITICAL PRODUCTION ALERT
          
          {{ range .Alerts }}
          Summary: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Severity: {{ .Labels.severity }}
          
          {{ if .Annotations.runbook_url }}
          Immediate Action Required: {{ .Annotations.runbook_url }}
          {{ end }}
          
          {{ if .Annotations.dashboard_url }}
          Dashboard: {{ .Annotations.dashboard_url }}
          {{ end }}
          {{ end }}
          
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#critical-alerts'
        title: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          *Severity:* {{ .Labels.severity }}
          
          {{ if .Annotations.runbook_url }}
          <{{ .Annotations.runbook_url }}|üìñ Runbook>
          {{ end }}
          {{ if .Annotations.dashboard_url }}
          <{{ .Annotations.dashboard_url }}|üìä Dashboard>
          {{ end }}
          {{ end }}
        send_resolved: true
        
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: '{{ .GroupLabels.alertname }}: {{ .GroupLabels.service }}'
        severity: 'critical'
        details:
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          source: 'Prometheus'
          component: '{{ .GroupLabels.service }}'

  # Security team alerts
  - name: 'security-team'
    email_configs:
      - to: 'security@company.com'
        subject: 'üîí Security Alert: {{ .GroupLabels.alertname }}'
        body: |
          SECURITY ALERT DETECTED
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          
          {{ if .Annotations.runbook_url }}
          Security Runbook: {{ .Annotations.runbook_url }}
          {{ end }}
          {{ end }}
          
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#security-alerts'
        title: 'üîí Security Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          {{ end }}

  # Database team alerts
  - name: 'database-team'
    email_configs:
      - to: 'database-team@company.com'
        subject: 'üóÑÔ∏è Database Alert: {{ .GroupLabels.alertname }}'
        
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#database-alerts'
        title: 'üóÑÔ∏è Database Alert: {{ .GroupLabels.alertname }}'

  # Business/Product team alerts
  - name: 'business-team'
    email_configs:
      - to: 'product@company.com'
        subject: 'üìä Business Metric Alert: {{ .GroupLabels.alertname }}'
        
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#product-alerts'
        title: 'üìä Business Alert: {{ .GroupLabels.alertname }}'

  # Platform team alerts
  - name: 'platform-team'
    email_configs:
      - to: 'platform@company.com'
        subject: 'üèóÔ∏è Infrastructure Alert: {{ .GroupLabels.alertname }}'
        
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#platform-alerts'
        title: 'üèóÔ∏è Infrastructure Alert: {{ .GroupLabels.alertname }}'